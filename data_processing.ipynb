{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T22:39:43.138128Z",
     "start_time": "2024-08-05T22:39:41.133532Z"
    }
   },
   "source": [
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchreid\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Load pre-trained OSNet model\n",
    "reid_model = torchreid.models.build_model(\n",
    "    name='osnet_x1_0',\n",
    "    num_classes=1000,\n",
    "    pretrained=True)\n",
    "\n",
    "reid_model = reid_model.to(device)\n",
    "reid_model.eval()\n",
    "\n",
    "model = YOLO(\"yolov8l.pt\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bogdanmatache/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/bogdanmatache/Library/Python/3.9/lib/python/site-packages/torchreid/reid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"/Users/bogdanmatache/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T22:39:43.141828Z",
     "start_time": "2024-08-05T22:39:43.139200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_people(frame):\n",
    "    bboxes = {'bbox': [], 'frame': [], 'conf_score': []}\n",
    "    result = model(frame, classes = [0], verbose = False)\n",
    "    for conf_score in result[0].boxes.conf:\n",
    "        bboxes['conf_score'].append(conf_score.squeeze().tolist())\n",
    "    for bbox_coord in result[0].boxes.xyxy:\n",
    "        bbox = (np.round(bbox_coord.squeeze().tolist()).astype(int)).tolist()\n",
    "        bboxes['bbox'].append(bbox)\n",
    "        bboxes['frame'].append(result[0].orig_img)\n",
    "    return bboxes"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T22:39:43.145327Z",
     "start_time": "2024-08-05T22:39:43.142467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity(a, b):\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def dot_product(a, b):\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "    return np.dot(a, b)\n",
    "\n",
    "feature_db = {}\n",
    "next_id = 0\n",
    "\n",
    "def assign_id(features):\n",
    "    global next_id\n",
    "    best_match = None\n",
    "    best_score = -1\n",
    "    for id, db_features in feature_db.items():\n",
    "        score = cosine_similarity(features.detach().cpu().numpy(), db_features)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = id\n",
    "    if best_score > 0.65:  # threshold for matching\n",
    "        return best_match\n",
    "    else:\n",
    "        feature_db[next_id] = features.detach().cpu().numpy()\n",
    "        next_id += 1\n",
    "        return next_id - 1"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T22:39:43.148479Z",
     "start_time": "2024-08-05T22:39:43.146056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(bbox, frame):\n",
    "    image = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (256, 256))\n",
    "    image = image / 255.0\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    image = (image - mean) / std\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    image = torch.FloatTensor(image)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    return image"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T22:39:43.151453Z",
     "start_time": "2024-08-05T22:39:43.149767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_features(model, image):\n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "    return features"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T22:43:08.237721Z",
     "start_time": "2024-08-05T22:39:43.152017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cap = cv2.VideoCapture('scene_example.avi')\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "out = cv2.VideoWriter('annotated_video_80.avi', cv2.VideoWriter_fourcc(*'XVID'), frame_rate, (frame_width, frame_height))\n",
    "inference_results = []\n",
    "\n",
    "frame_data = {'frame_nr': [], 'bbox': [], 'id': [], 'orig_img': []}\n",
    "\n",
    "with tqdm(total=num_frames, desc=\"Person Detection\") as pbar:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        id_list = []\n",
    "        orig_img_list = []\n",
    "        frame_nr_list = []\n",
    "        bboxes_list = []\n",
    "\n",
    "        frame_nr_list.append(int(cap.get(cv2.CAP_PROP_POS_FRAMES)))\n",
    "        bboxes = detect_people(frame)\n",
    "        for bbox, frame in zip(bboxes['bbox'], bboxes['frame']):\n",
    "            image = preprocess(bbox, frame)\n",
    "            features = reid_model(image)\n",
    "            id = assign_id(features)\n",
    "            id_list.append(id)\n",
    "            orig_img_list.append(frame)\n",
    "            bboxes_list.append(bbox)\n",
    "            cv2.putText(frame, f\"ID: {id}\", (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "            cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)\n",
    "            frame_data['frame_nr'].append(frame_nr_list)\n",
    "            frame_data['bbox'].append(bboxes_list)\n",
    "            frame_data['id'].append(id_list)\n",
    "            frame_data['orig_img'].append(orig_img_list)\n",
    "        pbar.update(1)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Person Detection: 100%|██████████| 718/718 [03:25<00:00,  3.50it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T22:43:09.056283Z",
     "start_time": "2024-08-05T22:43:08.668561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "frame_df = pd.DataFrame(frame_data)\n",
    "frame_df.to_csv('frame_data_80.csv')"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T22:43:08.662636Z",
     "start_time": "2024-08-05T22:43:08.656827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"for ids, bboxes, orig_img, frame_nr in zip(frame_data['id'], frame_data['bbox'], frame_data['orig_img'], frame_data['frame_nr']):\n",
    "    for id in ids:\n",
    "        print(f\"ID: {id}\")\n",
    "        person_frame = orig_img[ids.index(id)]\n",
    "        bbox_coord = bboxes[ids.index(id)]\n",
    "        person_frame = person_frame[bbox_coord[1]:bbox_coord[3], bbox_coord[0]:bbox_coord[2]]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(person_frame)\n",
    "        plt.title(f\"Person ID: {id} - Frame Number: {frame_nr}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for ids, bboxes, orig_img, frame_nr in zip(frame_data[\\'id\\'], frame_data[\\'bbox\\'], frame_data[\\'orig_img\\'], frame_data[\\'frame_nr\\']):\\n    for id in ids:\\n        print(f\"ID: {id}\")\\n        person_frame = orig_img[ids.index(id)]\\n        bbox_coord = bboxes[ids.index(id)]\\n        person_frame = person_frame[bbox_coord[1]:bbox_coord[3], bbox_coord[0]:bbox_coord[2]]\\n        plt.figure(figsize=(8, 6))\\n        plt.imshow(person_frame)\\n        plt.title(f\"Person ID: {id} - Frame Number: {frame_nr}\")\\n        plt.axis(\\'off\\')\\n        plt.show()'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T22:43:08.666061Z",
     "start_time": "2024-08-05T22:43:08.663839Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
